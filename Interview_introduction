Interview introduction
PaaC==> defining jenkinsfile in a code is pipeline as a code. A code is  anything that could be in the form of a file.
Iac ==. defining infra  is a file
name:
years experience: I am a devOps/cloud eng with over 5 years exp
 
I have worked on diff projects and with diff teams.
 I have worked with a team of 11 persons in an agile methodology . In order to better explain my experience I have 3 projects I will like to share with you 
    so you see how they ties  with the role required

1- When I was working with BDO Unibank (The first project is a project in the banking sector)

      This project took me 10 months to deliver and we were in a team of 5 persons
           They had an old system of observability that was posing issues with alert fatigues. 
           The system was creating a problem of visibility on the logs and matrices in real time. 
           It was difficult to handle incidents when they arise because there was no actual visibility. 

  The aim was to put in place a monitoring and observability tool which ensures matrices are tracked on time,
  anticipating and catching errors that could occur before the users of the application see them and solve them within reasonable time.

  Together with the team, we had to look at the structure of the org, look at the kind of platform that were using. They were using a k8s cluster for a micro service app. 
  I recommended Prometheus because Prometheus is good for k8s. It is dynamic and can be able to discover services and targets without having to configure and provision it all the time. 
  I was able to install Prometheus in the cluster using helm charts, I  was able to set indicators

  with this matrix, I was able to set a threshold if exceeded, alerts will be sent, and the process will move forward.

   Putting this in place, we were able to realise/deliver this project
     1- I was able to reduce the alert fatigues at a considerable level
     2- I was able to bring visibility on the cluster with respect to the matrix which was not the case before
     3- Incidence reolutions was done in a timely manner which made the system to be more reliable and performant.

      FOLLOW UP QUESTION  
     1- what are those key metrics (something happened)
                          - logs ( What happened) using Flentd which aggregates, filters, and forwards logs from various sources to destinations like splunk, Kafka, S3, Grafana, cloudwatch, Datadog
                          - traces (open Telemetry) ( where it happened) (flow of request from  browser and back)? 
                          - Events: they are time stampt and shows details of what has happened
                              - This will help optimise RTO ( Recovery time objective :  a key metric in disaster recovery and business continuity planning.
                                                             It represents the maximum acceptable amount of time that a system, application, or process can be 
                                                             offline following an unplanned disruption before causing significant harm to the business )
                                                       RPO ( Recovery point objective : is a crucial metric in disaster recovery and business continuity planning. 
                                                           It defines the maximum acceptable amount of data loss measured in time; in other words, 
                                                           it specifies how far back in time data can be recovered following a disruption )
        - cpu utilization: to be able to observe resource that are being consumed in case it exceed the threshold I should be able to provision more resources
        - memory utilization and ( if the % of used disk space > 80% fire and alert) total - used/total * 100
        - network bandwidth: To be able to determine the quantity of data that can be proceessed
        - disk usage (system memory metrics will enable to know the performance of the system). If the memory is over utilized the applications that are running will be slow
        - latency : 
        - number of request: the average traffic on an application per minute could be 3000 to 4000 per sec/min depending on the app
        - number of errors: for every 5 minutes, if the total number of request have 5% 400 errors then send an alert
        - I also monitor logs to be able to have observability of app that is running because the matrix tells something is wrong, but logs will go deeper to 
            have a clear situation by collecting the logs through a detail analysis of the logs.
           if a pod is crashlooping, if you describe the pods you can see whether its the image
     2- How did you monitor 
        - linux servers: node exporters
        - K8s : kube state metrics
        -  service/3rd party app: exporters and clent librares e.g python client library for python app
        - containers: C-advsors 
        by using node exporters deployed in the helms chart
     3- What is alert fatique/nuissance ?
        - matrix used are not right matrix alerts that don't prompt any action ( i was able to fine tone the matrix so that alert that are coming in are 
            ones that could promt action)
        - Sometimes I can be working on an error alert but the alert keep coming in
     4- How do you manage alert incidence? How to resolve an incidence
        - when there's an incidence I check my monitoring tool. 
        - Then I analyse  the impact of the incidence on the system, I check the logs, I run some queries.
        - Then I document what has happen.
        - I inform the different parties that the incidence is affecting by making them know the situation that has occured and we are in progress on solving it.
        - I proceed to plan how the issue is supposed to be resolve with the team (those with the right expertise to be able to solve the issue).
        - Ones the issue is reolved I document the process that was used to resolve it so that if it happends again there will be a documentation to follow how 
           it can be resolved in a timely manner.
        - The ticket will then be closed.
        - I proceed to do an indepth analysis to understand what caused the incident (post motem), How can we prevent this incidence from occuring in the future
     e.g we had an app that was running very slow and it was impacting the users heavily, after analysing my monitoring tool I realized that a data base querry 
          which was not well optimized was overloading our database therefore causing the app to be slow. we first put in place an emmergency catch to catch 
          the data while moving to optimize the querry by adding and index and everything was stabilized and the app started running

         Impact of the project
         -  I was able to resolve incidence on time and made the system to be more reliable and performant
      5- What aspect permited you to monitor pod/app matrix
         I used client liabrie
      6- what criteria did you use to configure or monitor a metrics and set alert?
        - latency : because its a metrics that can affect the performance of an app which the users see to be very important. Together with the team we had to see what is the SLA, SLO e.g 96%. AN indicator for latency could
                  be number of transactions processed per second=5000 within 3 min
                  threshold if no of trans pros per second= 6000 within 3 then alert 
                  96% meaning error budget of 4% (the down time that customers can allow for the app) this possibility to test new features and new things. if it falls below then there s instability in the system and no need
                   to release  new features.
        - I'm looking at success rate of app accessed:
        - Number of 500 errors e.g 5% within 5 min 
      7- observabilty is to  reduce the MTTI (mean time to investigate) and MTTR ( mean time to recover). You want to investigate what has happened very fast and also recover from what has happened very fast.
                  - SLO = 
                         - 99.9% of login requests succeed in < 300ms
                         - < 0.1% error rate for API requests
                  - SLI = service level Indicator
                  - SLA =
      8- How does prometheuse collect metrics from the target?
               - by adding the targets to the prometheuse config file ( add job name and target endpoint). After adding you need to refresh it by (Killall -HUB)
               - For dynamic targets like on the cloud where we have containers and pods that keep on changing use service discovery feature in prometheuse to discover any service and add them to the
               - role config and global config.
     9- How do you reconcile data when you have alot of data and the time series data base has reached it limit?
         I export it to a remote storage on the cloud like S3 , via the plugin which prometheus has which supports external API to be able to attach it
     10- how do you monitor linux machines that are not running as k8s clusters?
           - Install prometheuse on its own server  
     11- How can you specify nodes where a playbook should be executed?
          - by using the "when" directive
               

2- In another project which took about 12 months to realize. The team was 7 persons in an agile methodology mode. This was with IBM.
    The client had a pipeline that was not automated. A lot of things were done manually. 
    
          - Deployments were done with a manual trigger. 
          - Testing was done manually. That resulted in slow release of the software app.
     
    The aim was to put in place a pipeline for CI and CD that will be able to automate the process from end to end. 

   - I was able to automate the process by making sure that the pipeline was connected via a webhook that will trigger automatically upon a commit or pull request .

   - within the pipeline I had to integrate testing; static code analysis with Sonarqube, unit test (Jacoco) when code is written in java,
        integration testing  ( using test automation tools like selenium for webapp, Postman for microservices, python for API). This done in Dev env 
         and User acceptance within the pipeline using test tools like Junit and selenium. 

   - From there, at the level of security, I had to scan our image in the pipeline. I introduced Trivy to scan images and SonarQube for vulnerability detections.
      I also used Sneap for vulnerability detection. our pipeline was configured, and it was working as expected. 
          
          - I was able to deploy apps and 
          - I was also able to use the pipeline to provision our servers where I wrote Terraform Config files to create infrastructure . 

      All the infra that was used to deploy our apps, ec2 instances, and all the networking were created via terraform, 
      which made us to use and have a history of the infras and this permitted us to be able to manage them appropriately. 

     This project was successful even though there were challenges, but we worked together, which increased the speed of deploying our app. 
     I could have a state management of infrastructure by knowing which infra is running in our prod that gives us the possibility to know whether we can 
       ameliorate them or not.

          FOLLOW UP QUESTION
     1- What triggers a pipeline
         - a commit
         - a pull request
     2- What is CI
         It is  the process of pushing small pieces of code (functionality) regular (once a day) into the code base
     3- What was your pipeline design to do? 
         It was design to build an artifact which is a zip/tar file which will need to be tested to see that it is working as espected.
         Then it is going to be packaged and build into an image then pushed into an image registry.
     4- What kind of test was integrated into the pipeline?
         - unit testing e.g Junit or jacoco: tesing the function individually . logging
         - code scanning : sonarqube
         - Integration testing: tesing the different functionalities together ( logging and search tested together in the dev or testing env). This is done through a script
         - End to end testing: also UAT how the functionality will behave from the end user pespective. Also done through a script 
         - Prod  :
     5-  How to secure pod of jenkins due to security issues?
             /etc/default/jenkins  -- it is  the settings file
     6- what is the default path of  the jenkins home directory
            /var/lib/jenkins
     7- What are the different ways to install jenkins
          - a docker container
          -  apt get jenkins
          - k8s container via helm charts
          - as a standalone flie
    ( apt an apt package manager it comes with a jenkins file already configured, it comes with a log rotation file already configured, it has a setting file already involved )

     8- what are the 2 versions of java which jenkins support
             - java runtime 11
             - java runtime 17
     9- What is the file extension for jenkins plugin ?
              hpi
     10-  How can you optimze jobs in your pipelin if the pipeline is running slow?
          - by running 2 jobs in parallel if there are steps that don't depend on each other.e.g if you have a test that you want to do for an app that has to run on 2 OS linux and windows. since the are not depending
             on each other.The same test can run on linux and windows env(agents) that will speed up the pipeline.
          - I can make sure that I am cloning a single branch in a repo and not the entire repo because it will take time to download the dependencies since cloning the entire repo will take alot of bandwidth
             ( --single branch -b "branch name" git repo )  or download the repo and store in the local so that when you are running your pipeline it will available in a kind of local catch and it will not take alot of bandwidth
              again
          - To be able to see the time it takes to clone a repo put "TIME" infront of "git clone"
          - I clean old builds
     11- What is an agent in Jenkins?
           - It is the excutor of the job
     12- Which kind of agents have you used ?
            - linux agent
            - k8s containers/docker container : they are the best because they will be created when the job is scheduled and when the job is terminated they will also be terminated.
     13- In your pipeline do you do continous deployment or continous delivery?
           - Continous deployment is when the deployment phase is automated from CI to CD while continous delivery is when it is not automated because we want to check what goes to prod.
           - Continous Deployment can be used in the TESTING ENV because you are just doing test but when it comes to PROD you use continous Delivery. 
   
     14- What are some of the agents used to run jobs?
              1- containers
              2- linux agents
              3- windows
              4- EC2 instances
     15- What is the difference between continous Delivery and continous deployment?
              continous deployment is when the pipeline is automated
              continous delivery is when delivery is done manually because we want project owners to check what goes into production first
     16-  A pipeline is defined as a series of steps or stages that define the process of building, testing, and deploying software.
     17- Jenkins also have JENKINS WEB INTERFACE used for 
              - Configure jobs and pipelines.
              - Trigger or schedule builds.
              - View build status and logs.
              - Manage Jenkins system settings.
     18- what are the different ways in which someone can be authenticated in Jenkins?
           - I can use unix user or grp data base
           - Jenkins own data
           - external auth like LDAP
           - selvet if jenkin's is  running on an app like tomcat then I can use it to be able to auth.
             after auth i authorize by giviing access to specific jobs or specific projects
     19- How can you ensure security in a pipeline when running a build?
          - check the mark up formater making sure it is in plain text and not HTMl format prevent hacker from intercepting
          - change the TCP port 50,000 which enable jenkin's to connect to agents
          - I can change the default port which Jenkin's listens to in the /etc/defaults/jenkins to any port
          - Enabling cross-site request forgery ( CSRF) this is when someone who has been authenticated is being forced to unknowingly submit a malicious request to a web application they are authenticated with.
          - use folders to prevent team from seeing other teams work.
          - when jenkins wants to communicate with my k8s cluster or image registery I used the credential plugins that jenkins use to be able to manage credentials
     20- Why is it not advisable to run jobs on the jenkin's controller?
         - It is not good to do so because all jobs running run with administrative previlages. The person can get access and delete some sensitive data (credentials, secrets, or logs) that can jeopadize your workload
         - If you have a controller and  you are running jobs on it, the jobs are stored in the jenkins home directory which normally is a disk and with time it becomes so overloaded 
            (consumes CPU, memory, and disk I/O, which can degrade the performance of the entire Jenkins instance)
         - The controller is the brain of Jenkins. If you run jobs on it and a job causes it to crash (due to excessive resource usage or faulty scripts), the entire Jenkins environment goes down.
         - Jenkins controllers are not meant for running multiple builds. Running jobs on the controller limits parallelism, which reduces the efficiency of your CI/CD pipeline.
            Hence it is neccessary to use EC2 instance and containes to run jobs and build them.
     21- How were you interacting with Jenkins?
          - via the jenkins console (user interface)
          -  using the command line interface
          - jenkins API
     22- How did you configure notifications? if a job failed or succeeded
          - via slack
     23- How are you able to move your pipeline from the Dev env to the UAT, PROD?
          - via a pull request: the pull request is done because you have different branches (Dev branch, UAT branch, PROD branch)Normally the first branch is merged into the master, when it runs in the DEV env I check and see
             that everything is okay. Then I have to do a PULL REQUEST to merge the UAT branch into the MASTER BRANCH. Ones the pipeline run we are going to veriify the app in the UAT branch to check for integration testing,
             there we do END TO END TESTING to see that everything is okay thhen its approved. We merge the next branch into the master that goes into production.
     24- How many reviewer do you have to be able to viiew and confirm everything s okay.
          - superior or tech lead
          - Manager
     25- How did you ensure you 2 controllers were sync ?
         - I ensure that the home directory which is actually where all the jobs are stored _/jenkins_home) should be synced using an storage like a network attached storage which makes sure that they are syncronized in case 
           one of the controller is down, the active becomes the passive and the passive becomes active.
     26- How were you able to load balance job in the 2 controllers since they were in an active passive mode?
         - used a reversed proxy which sits on the 2 controllers to be able to route traffic if the controllers were runnng on premise machines but if they are running on the cloud the I use a loadbalancer.
     27- How was connection intated between the controller and the agents?
         - for linux agent jenkins initates the connection via SSH
         - for windows agent , the agent initiates the connection
     28- While using docker as build agents were you using images that were already avaiilable or you had your own image that you were to buld to run the job and in which situation will you use them ?
         - we images that are already available that have all the dependences e.g if want to run a maven project I can use a maven Eclipse Temurin image that has all what is needed to buld. I can use it, it will run
           build a container and at the end terminate the container
         - In a situation where i have a project or app that have dependencies that are not available the I should be able to write my own Dockerfile having all the dependencies in it. Use it to be able to build my image
           upon the running of the pipeline, that image will be used to build the container where that app will be used
     29- Can you run a freestyle job as a pipeline job?
         YES, I can chain them using upstream and downstream project where one job runs and triggers another to run e.g build A if the other is successful.
     30- what is a shred library and when is it feasible to use iit?
        - a shared library permiit collaboration among teams because it removes repetition.
        - It helps us to be reuse our pipeline because modification can be done on one place and then the same pipeline can be used in a different env ( DEV, STAGE, AND PROD)
          we can import the shared library into our jenkins file using @library "name of shred library" _
     31- Example situation where I have used a groovy script to decongest the pipeline?
        - stages where I have  alot of shell commands that are  passed in the Jenkins file whiich makes my pipeline so complex e.g at the level of the pipeline with docker we were building and image , we had
          with docker  build, docker tag, docker push. All these aspects in the same stage made the pipeline to be so congested. I was able to remove them and put in a groovy script and incase there is any modification I
          do it in the script. This permits not to be  mdifying our  Jenkinsfile  

 3- The 3rd project the company had a legacy app. 
        - The app had a problem to scale for increase in workload.
        - It was difficult to maintain the app, 
        - and additional features were not able to be released on time. 
        -  
  Due to these there had to be a refactoring of this app into a micro service.
 
   - In this situation, I was in charge of designing the architecture for k8s to be able to make sure that those app which were refactored were going to run there. 
     Since The company was strong in AWS cloud so we had to use an EKS cluster but the aim was to make sure that we are using a managed cluster to have control of our worker nodes. 
     
         - I designed the cluster,
         - regular updates were made on the cluster, 
         - wrote manifest files for deployment via helm charts, implemented replicas for high availability of app. 
       
     The k8s cluster was deployed in 2 availability zones, I was able to use the idea of ingress to be able to expose our app externally.
 
   - I ensured security of our app on the cloud where our clusters were running using Network Access Control List and using SG. In the cluster I was able to use RBAC  for security,
     I put in place a network policy to disrupt pod to pod communication.
   - Our containers that were running were harden containers that did not have root access to reduce any attack surface. I equally  
   - I made use of C-grp and linux namespace to separate file processes.
   - I configured ASG which were connected to my ec2 instance to ensure scalabilty in case of increase workload
   - I was able to put in place Pv, pvc for persisting of data among pod restart because  

      FOLLOW UP QUESTIONS
     1- Terraform files are configuration files ( manifest files are written in yaml)
     2- WHAT BACK END DID YOU USE FOR YOUR PV?
          - What is the hostpath: data is persitent on the local machine but it is not a good idea
          - you can use seph
          - you can use network file system (remote file system) because the storage does not depend on your laptop.
          - Google persistent disk 
          - s3 bucket, 
          - EBS volume
     3 - why did you use a readinessProbe ?
         To make sure that user request were sent to pods that are ready. If you don't do health check request will be directed to pods that are not ready 
         leading to poor user experience.
