  1 Tell me about yourself.
  1- name
  2- Knowledge
  3- Interpersonal skills

A - How do you authenticate yourself/user in Jenkins?
      - manage jenkins--> security--> 
        1- Jenkin's own user database
        2- By using a delegate to servlet container
        2- You can use LDAP ( light director access protocol)  
        4- Unix user/group database. 

          Authorization
      - what the users can do 
         -  anonymous users can do anything
         - legacy mode-same with  admin role / full access
         - logged-in users can do anything- full access except anonymous users who only have read access
         - matrix based security :It is the best method.  here we have anonymous user and authenticated user and we can aothorize them to do varied things 
             ( overall, credentials, agent, job, Run, view, SCM -source code management, metrics)
         - we also have CSRF ( cross side request forgery) Protection :  
         - Project-based matrix authorization strategy: if you want to narrow it a each project separetly e.g Joe can access project A, B and C but he can't see D

      - we equally have a mark up formatter ===> used for security
         - when running jenkins it should be in plan text and not safe HTMl so that harcker can intercept
         - You can equally change the TCP port for inbound agents which is 50,000 . This port is used to connect the jenkins controller to the agent. You can disable it to make it more secured,  or use the Random port 
         - from default port 8080 to any other port in the   /etc/defaults/jenkins file
B- How can you run docker commands in  jenkins conatainer?
      - By mounting the docker volume
      - docker run -d-p 8080:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/usr/bin/docker jenkins/jenkins:lts
         - jenkins_home:/var/jenkins_home ( everything you installed on jenkins is persisted on this volume on the host) name volume. You can equally check all jobs run on jenkins here
         - If you run your pipeline and see error like docker command not found  ===> it means the docker.sock file  has lost its executable command.
         - jenkins/jenkins:lts   ===> This is jenkins image
      - Jenkins  can be  able to run k8s command when it will be granted permission (authenticated as a service account, create roles, then we bind the roles) as a service user  
2- K8s
  1- Describe the k8s architecture
    1- It is made of Master nodes
       - API server - exposes all the api calls; its the entry point to the cluster, it authenticate any user, authorizes and then admits 
       - kube Scheduler. It is used to schedule pods. 
       - etcd. Its the cluster data base. It's a key value pair that is used to store all the state of the cluster written in go language
       - controller manager. It controls the cluster, looking at the desired state and the actual state and is able to reconcile them.   
    2- Worker nodes
      - kube proxy. It is used for networking
      - kubelet. It is used for communication
      - container runtime. It provides the underlying infrastructure to run containers
  2-SECURITY IN THE K8S CLUSTER
     1- RBAC
      - Authentication
      - Authorization
     2- NETWORK POLICY to control communication btw pods and nodes to provide cluster firewall to prevent open
          communication
       - Container security
          regularly scan images before they are being deployed in the cluster
       - Management of secrete
          secretes can be managed in Hashicorp Vault or AWS KMS since k8s does not have a service that can encrypt secrets except for encoding
     3 - You can equally do audit logging. You can log your cluster to get activities of what is happening in your cluster. In case there's an issue you can do that.
     4 - Regular update and patching of the cluster since every 2-3 months a new version is released and you need to be doing regular updates as a security measure
 
  3- What happens to a cluster when a master node fails
   - For high availability of your app you should have 2 or 3 master nodes in the different availability zones, but if you have just 1 and it fails. Nothing will happen to the cluster.
    The app will still be running on the nodes as expected, but you will not be able to control the workload in the cluster. If you want to scale, you cannot.
    If you want to deploy, you cannot because the master node cannot make an API call.
 
  4- What happens when a worker Node dies or stop working.
    The control manager in the master node will look at the desired state  vs the actual state ( which are pods managed by a deployment and replicas) and recreate them on other nodes available in the cluster. 
  5- What is livelinessProbe and ReadinessProbe

  6- What is an init container
  - Generally, you have a pod that could be running 2 containers and you have an init container running beside the main container. In such a situation, 
    the init container is first going to check the environment to see whether it is ready before the main container starts running. It can download dependencies

  7 - What is a daemonset
  -  It is a kind of k8s object that ensures that pods are deployed on individual nodes.
  - This is practical in a situation where u are deploying a monitoring tool like prometheus. You are using a helms chart to deploy prometheues inside your k8s cluster. 
     You need to have the prometheuse on all the cluster so that it can help to collect the metrics and be able to ensure monitoring.

  8- what are different services in k8s
    - ClusterIP
    - NodePort
    - Load balancer but will need cloud provider
    - External namespace
        -Ingress controller
  9 - Is it possible to schedule a pod on a node that is tainted?
   yes, if the pod has a toleration.
    A toleration specifies that a pod can ignore a certain tainte so that the pod can still be scheduled on the node. A tainte has a key value and the Toleration Effect.

  10- How can jenkins be able to run kubectl commands inside the k8s cluster?
       - since jenkins is a service we are going to 
          1- Authenticate jenkins using service account
          2- Authorize  jenkins using RBAC

3- DOCKER SWAM
     - it is not very robust
     - It does not have capacity to scale
     - it does not allow addons while k8s you can do that via operators
   B- what are some of the best practices you have implemented when writing a docker file?
      - make of use of the .dockerignore to exclude files which are not very important for the context build.
      - use multi stage build to seperate the Build from the RUN
      - I can use multiple && to chain multiple commands  into a single RUN statement, creating fewer layers.
      - make use of a light weight image like Alpine
  c- why use && when writing a docker file? operators in RUN instructions
    - Combining commands with &&,prevents image Bloat  allows you to clean up temporary files in the same layer, reduces image size by removing unnecessary files.
    - Speeds up builds by improving caching, Docker uses layer caching to speed up builds. If you change one line, Docker invalidates the cache for that line and all subsequent layers.
    - Ensures reliable execution by chaining dependent commands.
    - The && operator ensures that the next command only runs if the previous one succeeds (returns exit code 0)

4 - WHAT HAVE YOU BEEN ABLE TO DO WITH ANSIBLE
    - It is a tool used to do automation work like 
      1- configuring of servers: install ansible , configure inventory file, test connectivity, write the playbook(), run the playbook 
          (ansible-playbook -i /etc/ansible/hosts playbook.yml
      2- deploying apps (Install ansible on the controller, define my inventory file(/etc/ansible/hosts), write the pplaybook(deploy.yaml), run the playbook)
      3 - provisioning of infra
   -  With the use of ansible playbook you can do a lot of work. You can use Roles to make sure that the playbook is reusable on different envs.( ansible-galaxy create "role name" )
  - Ansible works with the use of modules. Modules are tasks or small programs that is executed on the host . 
     * You could have the 
          file module,==> is used to manage file system objects like files, directories, and symbolic links on remote hosts. It helps in tasks such as creating, deleting, modifying permissions, setting ownership, and managing symlinks.
          copy module, ==>  It is mainly used for transferring simple files, setting permissions, and ensuring file presence. Ensures idempotency (only updates the file if changes are detected).
          template module,==> Generate config files like nginx.conf, Secrets Management: Store sensitive information as templates and populate them securely at runtime.
          lineinfile module, ==> Modify a specific line in a file
          service module ==> manage service
          command module ==> run commands on remode nodes
          cron module ==> to mande cron jobs
          fetch module ==>  Fetch files from remote servers
          replace module ==> replace text in a file
          stat module ==> retriev infos about files 
          ping module ==> Check connectivity with remote hosts
          firewalld module ==>  Manage firewall rules in Firewalld
          archive module, 
          register module, 
          get url module (to be able to download file), 
          and shell module. ==> to run shell scrypt
          k8s ==> Manage Kubernetes resources
          docker_container ==> Manage Docker containers
          docker_network ==>  Manage Docker networks
          helm ==> Manage Helm charts
  5-  which module does  not respect the  principle of idempotence in ansible?
        - Command Module         user module
        - shell module
  6- How many Playbooks should run
         1 playbook can be deployed on by default 5 machines
  7- Run a Playbook in a dry run mode
          --check       will run the playbook on a dry run mode
  8- How to skip certain task in a playbook
       -- skip tags  ( to skip)
           tag   ( to include)
  9- How do you test a playbook when you write it?
       - you could use molecule or a lint to test the playbook
       - running in on dry run
  10- How do you run your playbook in production?
     - It should be running in a pipeline. You trigger a pipeline to run the playbook and execute where it is supposed to since the playbook should be kept in a source code repo
  11- How can you increase the number of playbooks to run in ANSIBLE ?
      - you can use a fork: fork=20
         ansible.cfg file
     - For servers that are not running: you use dynamic enventory which permits to add inventory in the host file
             - We have static inventory (A static inventory is a manually maintained file that lists all the managed nodes  written in YAML or JSON)  
               and Dynamic Inventory (A dynamic inventory is generated in real-time by querying external data sources, such as cloud providers, CMDBs, or other APIs .
             - an inventory defines the hosts and groups of hosts upon which Ansible commands, modules, and playbooks operate. 

  12- In which language is ansible written?
      it is written in python
  13- What is a Playbook?
       It is a set/list of instructions/task/ plays that have to be executed on managed nodes.
  14- By default what strategy does ansible use to excute the playbook?
      Ansible uses the linear strategy: for example if the playbook has to run on 10 servers it will execute stage1 on all servers before it can proceed.
      you can change it to parallel strategy
  15- To reduce the time of execution of ansile ?
      - you can disable to gather facts ( gather fact: false)
      - change the default execution of the playbook from linear to parallel
  16- To know the time of execution of a playbook
      You use the TIMER
  17- What is a role in ansible?
     It permits to seperate the playbook and run it in different env. You can create it using ansible_galaxy
  18- What is ansible Tower?
   - It is the enterprise version of ansible which comes with a user inteface which enable you to be able to control all the managed notes from the user inteface and you see them. 
     You  can know which  playbook failed, which playbook is still running, it gives full visibility and can permitt you to run your playbook across different teams
  19- Why Use Ansible for Provisioning?
       - Agentless (uses SSH)
       -  Supports multiple cloud providers
       - Idempotent (ensures consistent state)
       - Easily integrates with CI/CD pipelines
  20- what are sojme of the playbooks you have written?
     - I have writtena playbook to deploy a docker compose
     - a playbook to install nexus
     - a playbook to install jenkins
     - a playbook to add users to a system and delete users
     - 
  21- which module permits us to store variables to be able to use in another?
      - The register module:
  22- If you have a variable on the variable files how do you pass that on the playbook?
       --extra vars "name of variable"
  23- How do you solve the problem of idempotency in a playbook?
      if you want to create a file which has already been created you use ( --create ) the command will not be executed if the file exist, and  (--delete) will not delete any file if file does not exist.
  24- how do you handle secretes in ansible?
      by using ansible vault : it can be used encrypt the playbook ,it can be used to keep the keys,manage secrets , SSH keys, 
  25- when you write a playbook where do you keep it?
      Since it needs to be versioned it needs to be kept in a SRC repo and configure a pipeline to run the playbook.

5- WHY DO YOU NEED OBSERVABILITY
    Observability is very important to know the performance of our system and also before any issue occurs we should be able to have visibility of what is happening.. 
   - I have been able to monitor linux servers via node exporters
   - I have been able to monitor services/3 party app via client libraries and exporters
   - I have monitored containers via c- advisor
   - I have monitored k8s cluster via kube-state metrics 
  I was able to install prometheuse using helm charts and put in place Grafana dashboards to be able to visualize those metrics on Grafana dashboards. 
  - I have been able to send alerts to the different parties that are involved in case thresholds were exceeded.
       - firing
       - insufficient data
       - ok
  - I have sent alerts based on cpu utilization, memory usage, number of requests, and number of errors like 400 above a certain %. I've been able to set an alert for latency to know how fast my apps are responding.
   - I have been able to put in place system of logging to be able to help me know what is actually wrong with my app by consulting logs (fluend) and equally traces using open Telemetry to be able to have full visibility of the movement of the request of a client from when the request is sent and received back.

 6 - What is the git flow strategy
    - It is a strategy of maintaining 3 branches. If there's any bug, you can create the bug fix branch.
      if there's a new feature you create a feature branch. 
    - A pull is a formal approval from a superior to be able to merge a branch into the master.
    - A READme file is a file that carries information about the project.
    - To come back to a staged branch ( git starch pop)
    - git log permits to see the commit history.
    
 7- LINUX 
     - Some commands used to check network
        - nsloolup for dns resolution
        - netstat -tlup
        - ping for
        - dig for dns compliance 
    - to check users in thee system 
          /etc/passwd
    - To check resources that the system is running
           df -h, free -m
    - What is swap in linux?
       it is used to recover memory. some apps that are running but no longer active
    - What is daemon?
      It is background process running without human intervention
    - How to check if a process/ service is running?
        systemctl status "name of service"
 
 8 - AWS
      IAM is an AWS service that can be used to control who has access to the cluster and who can do what. 
    - What is an S3 service
      It's an AWS service is used to store object 
    - How do you upload objects into an S3
        1- via console ( the max size is 160 GB)
        2- or command line ( up to 50 TB)
    - how many S3 buckets can you have per AWS account?
        100 but you can apply to have up to 1000
    - What is a bucket policy? How do you control access into an S3 bucket?
       1-By using an Access Control List.
         This is a kind of permission given to be able to access individual objects from the bucket.
       2- A bucket policy.
          A bucket is attached on the object in the bucket.
       You can replicate items on the bucket from one AWS account to another.
    - How do you secure data on AWS?
      1- By using TLS, and SSL that permit you to encrypt data on transit
      2- You can encrypt AT REST using server-side encryption or a client side encryption.
    - AWS serverless services
        - lamda 
        - API gate ways
        - fargates
        - s3
    - What is AWS LAMBDA?
       This a compute service that you can use to run code without provisioning or creating any server. 
       It is one of the fast way of running apps now because individuals don't want to carry the heavy lifting of managing infra. 

    - What is A LAMDA FUNCTION? It the app code that can be written in python, nodejs, java, C-sharp, go
      It is a code that have been written to permit the job to be done. A dev can write a code but they don't want to provision any server/resources to deploy the app 
       so they will upload the code to a lambda function.
      Ones the lambda functions takes the code, you need to define the memory that your app will use to run on and AWS will create an env taking into consideration
        the memory needed by the app, the CPU, the dependencies which it needs to download. 
      This env is also known as an execution env. Ones the env is ready that code will only run base on a trigger, then the a destination.
       An S3 bucket can trigger a lambda function. e.g pictures are dropped in an S3 bucket. Based on a PUT EVENT on an S3 bucket, 
      it's going to trigger a lambda function but for it to happen, the resource that needs to trigger the lambda functions needs a resource base policy. 
         There must be a policy that gives the S3 bucket permission to act on the lambda function. 
      The function will run based on trigger, convert the picture and drop it in its destination, which could be another S3 bucket. For it to drop it in the S3 bucket,
         the lambda function should have an execution role ( the action which the lambda function has on the resource). 
     - It could also be triggered by a synchro nose evocation is where, when the lambda function is evoked it will need to wait to get a response and send 
         it back but an asynchrony nose evocation is when the lambda function is evoked it goes and deliver without waiting for a response. 
  
9- Terraform
    - How do you create infrastructure?
        I use IaC to be  able to create infrastructure.
    - What is terraform show: 
         it shows all the  information in the state file
    - What are the elements of the hashicorp  configuration in the terraform  block?
        we use the terraform block to be able to configure our project.
    -  What are the differrent blocks used to configure resorces
        1- resource block
            - the type of resource
            - the label : used  to uniquely identify a resource
        2- Data Source Block
        3- Varables block : we can customize our variables which we want to use in our CF
           - varable name
           - type
           - description
        4- output Block : it's used to exposed information to the outside world
        5- The Local block : it permits to create a temporal variable inside a function.
        6- Module Blocks: It's a piece of a reusable code that can be imported in our current project 
            - they have labels
  10- In k8s what were you scaling?
      - you can scale the pods
      - you can scale the cluster
  11- What is vertical auto-scaling ? scaling up or down
       Involves increasing or decreasing the power of a single server or instance. Adds more CPU, RAM, or storage to the existing machine when demand increases.
       Examples: Database servers, monolithic applications. disadvantage is has  a limit to how much you can scale and may lead to down time when scaling up and down.
  12- What is vertical auto-scaling ? scaling in or out
       Involves adding or removing instances (servers) to distribute the load.
       Examples: Web applications, APIs, and stateless services

    Vertical scaling = Hiring a super chef to cook faster.
    Horizontal scaling = Hiring multiple chefs to handle more orders simultaneously.
13- what was giving you the metric server?
     cloudwatch was watching the servers and the metric server which was installed in the cluster was collecting the metrics about the CPU usage (kubectl top), alert manager will send the alert of the cpu utilization.
14- In which situation are you going to do a horizontal pod auto-scaling and in which situation are you going to do a horizontal cluster auto-scaling?
     - For horizontal pods auto-scaling you are scaling pods when there's increase in workloads and the pods are consuming resources ( we have min, max and desired) if the pods have used all the resources in 
       the cluster and cluster can no longer have resources because there's way they will consume resources then you scale the  cluster to create more worker nodes so that more pods  can be scheduled on the worker nodes
       until the horizontal auto-scaling has reached its max and there's still increase in workloads then you can proceed to scale the cluster so that you can have more worker nodes to schedule pods to handle incarese in traffic
       you start by scaling pods based on the replicas that have been defined. you have to scaled the pods first until the avearge resources defined in the  cluster have been exceeded then you scale the cluster.
15- what is white box monitoring?
    is having complete visibilty of the system internally and having and output of what is happening. you can use instrumentation to do a white box monitoring by including a prometheus code in the app 
    code base so that promethuesss can collect details of what is happening inside the app and you are able to have it externally
    while blank box montitoring, monitors the system superficially using tools like nagios
16- what  do pod use to communicate which each other?
    they communicate which each other using cluster IP.
17- How do 2 containers inside a pod communicate with each other?
    using a shared namespace to communicate with each other produces by a pause container?
18- Have youu had a situation where you had 2 containers running inside a pod?
    - init container
    - sidecar container
    - Ambassador container: it is a proxy container used to enhance communication wheree the maiin container does not have means to communiicate
    - Adapter contaiiner : used to formate metrics
19- How to you make sure that teams don't exceed their resources defined in each namespace?
    use resources request and resouces quotas
20- 
===================================================================================================================================================================================================================


