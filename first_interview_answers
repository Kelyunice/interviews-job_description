  1 Tell me about yourself.
  1- name
  2- Knowledge
  3- Interpersonal skills

A - How do you authenticate yourself/user in Jenkins?
      - manage jenkins--> security--> 
        1- Jenkin's own user database
        2- By using a delegate to servlet  if jenkins is runniing on an app like tomcat 
        2- You can use LDAP ( light director access protocol)  
        4- Unix user/group database. 

          Authorization
      - what the users can do 
         -  anonymous users can do anything
         - legacy mode-same with  admin role / full access
         - logged-in users can do anything- full access except anonymous users who only have read access
         - matrix based security :It is the best method.  here we have anonymous user and authenticated user and we can aothorize them to do varied things 
             ( overall, credentials, agent, job, Run, view, SCM -source code management, metrics)
         - we also have CSRF ( cross side request forgery) Protection :  
         - Project-based matrix authorization strategy: if you want to narrow it a each project separetly e.g Joe can access project A, B and C but he can't see D

      - we equally have a mark up formatter ===> used for security
         - when running jenkins it should be in plan text and not safe HTMl so that harcker can intercept
         - You can equally change the TCP port for inbound agents which is 50,000 . This port is used to connect the jenkins controller to the agent. You can disable it to make it more secured,  or use the Random port 
         - from default port 8080 to any other port in the   /etc/defaults/jenkins file
B- How can you run docker commands in  jenkins conatainer?
      - By mounting the docker volume
      - docker run -d-p 8080:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/usr/bin/docker jenkins/jenkins:lts
         - jenkins_home:/var/jenkins_home ( everything you installed on jenkins is persisted on this volume on the host) name volume. You can equally check all jobs run on jenkins here
         - If you run your pipeline and see error like docker command not found  ===> it means the docker.sock file  has lost its executable command.
         - jenkins/jenkins:lts   ===> This is jenkins image
      - Jenkins can be able to run k8s command when it will be granted permission (authenticated as a service account, create roles, then we bind the roles) as a service user  
2- K8s
  1- Describe the k8s architecture
    1- It is made of Master nodes
       - API server - exposes all the api calls; its the entry point to the cluster, it authenticate any user, authorizes and then admits 
       - kube Scheduler. It is used to schedule pods. 
       - etcd. Its the cluster data base. It's a key value pair that is used to store all the state of the cluster written in go language
       - controller manager. It controls the cluster, looking at the desired state and the actual state and is able to reconcile them.   
    2- Worker nodes
      - kube proxy. It is used for networking
      - kubelet. It is used for communication
      - container runtime. It provides the underlying infrastructure to run containers
  2-SECURITY IN THE K8S CLUSTER
     1- Authentication
      - Authorization via RBAC
     2- NETWORK POLICY to control communication btw pods and nodes to provide cluster firewall to prevent open
          communication
       - Container security
          regularly scan images before they are being deployed in the cluster
       - Management of secrete
          secretes can be managed in Hashicorp Vault or AWS KMS since k8s does not have a service that can encrypt secrets except for encoding
     3 - You can equally do audit logging. You can log your cluster to get activities of what is happening in your cluster. In case there's an issue you can do that.
     4 - Regular update and patching of the cluster since every 2-3 months a new version is released and you need to be doing regular updates as a security measure
 
  3- What happens to a cluster when a master node fails
   - For high availability of your app you should have 2 or 3 master nodes in the different availability zones, but if you have just 1 and it fails. Nothing will happen to the cluster.
    The app will still be running on the nodes as expected, but you will not be able to control the workload in the cluster. If you want to scale, you cannot.
    If you want to deploy, you cannot because the master node cannot make an API call.
 
  4- What happens when a worker Node dies or stop working.
    The control manager in the master node will look at the desired state  vs the actual state ( which are pods managed by a deployment and replicas) and recreate them on other nodes available in the cluster. 
  5- What is livelinessProbe and ReadinessProbe

  6- What is an init container
  - Generally, you have a pod that could be running 2 containers and you have an init container running beside the main container. In such a situation, 
    the init container is first going to check the environment to see whether it is ready before the main container starts running. It can download dependencies

  7 - What is a daemonset
  -  It is a kind of k8s object that ensures that pods are deployed on individual nodes.
  - This is practical in a situation where u are deploying a monitoring tool like prometheus. You are using a helms chart to deploy prometheues inside your k8s cluster. 
     You need to have the prometheuse on all the cluster so that it can help to collect the metrics and be able to ensure monitoring.

  8- what are different services in k8s
    - ClusterIP
    - NodePort
    - Load balancer but will need cloud provider
    - External namespace
        - Ingress controller 
           (which type of INGRSS have you used =NGINX INGRESS, HAProxy,Traefik,  AWS ALB Ingress Controller, Kong Ingress 
        - Layer 7 is an app layer support which provides routing based on host and path 

  9 - Is it possible to schedule a pod on a node that is tainted?
          yes, if the pod has a toleration.
    A toleration specifies that a pod can ignore a certain tainte so that the pod can still be scheduled on the node. A tainte has a key value and the Toleration Effect.

  10- How can jenkins be able to run kubectl commands inside the k8s cluster? 
       - since jenkins is a service we are going to 
          1- Authenticate jenkins using service account
          2- Authorize  jenkins using RBAC
   11- What is pod policy?
        Pod policy is used to control what policy can run inside the pod e.g containers should not run with root privileges?
   12- Which deployment trategies have you been able to use?
       - BLUE GREEN: this is wen you replicate the same env, and route traffc to both blue (active) and gree (thhe new) en and if everything is working as expected 
                  I switch the label and the selector and users now see only the Green
       - Cannary releases : 25 / 75 progressively thhis is so that if there's an issue everybody should not be affected
       - Rolliing update whhhichh is the default startegy in k8s.
   13- what is the difference between "kubectl create" and "kubectl apply"?
        - Kubectl create : this used to create a resource that will not need to be modified (e.g
        - kubectl apply : for resources that can be modified
   14- A namespace is used to segregate a cluster between team to better manage resources.( Pods, Deployments, ReplicaSets, StatefulSets,DaemonSets, servces, ingress,
                 configMaps, secrets, jobs, CronJobs, PVCs, Network Policies, ResourceQuotas, LimitRange, Role and RoleBinding) usng Kubectl get all -n "name-space"
   
   15- What is the difference between a role and a cluster role?
        - A role is limited to a namespace
        - A cluster role is for the entire cluster. Admins have cluster role.
   16- How do you upgrade your cluster?
       - i use a Rolling update strategy meaning one-node at a time.
       - I start by draining  the nodes makiing them unschedulable " kubectl drian "name of node" --ignore deamonset so that only core DNS pods and networking pods should be running like calico
       - evict the nodes
       - upgrade to the required version.( the kubect and kubelet version should not be more than 1 minor version from the k8s control plane because Kubernetes follows a version skew policy
         to maintain stability, compatibility, and reliable operation across its components. 
             - API compatibility: If they're too far apart, the older client may not understand the new APIs, or new clients might call APIs that donâ€™t exist yet.
             - stable communication: kubelet talks directly with the control plane (API server) so  if  they are far apart it might Misinterpret configs (e.g., scheduling or resource limits).
               e.g control plane 1.29
                   kubectl = 1.28, or 1.29, or 1.30
                   kubelet = 1.28, or 1.29 but  not 1.30
       - ones done, I unconrdone the node making schedulable again but pods which were not manage by a deployment or replica will not be scheduled again ones they are evicted.
   17-  how can you make sure an app or deployment to run on a specific node?
         - direct pin ( under spec, nodeName="")
         - node affinity
         - taint + toleration to the pod (kubectl taint nodes my-node key=value:NoSchedule) (pass the toleration in a yaml file with "key value"  and "tainte effect")
   18- How do you ensure that each pod to be scheduled on each node. you don't want 2 of those podes to be scheduled on the same node?
       - pod anti-affinity
  19- how do you expose your app,explain the process?
       -  Ingress provdes a layer 7 loadbalancing to be able to expose app externally, SSL termination. It is deployed insiide the cluster using helm and I put an ingress resource that wiiill be defining how to route that
            traffic to the different backend services.
       - Layer support routes traffic to multiple backends eiither based on the host or based on the path  (ALB , ingress)
   20- what is the dfference between ALB and NLB?
       - An ALB route traffic based on http and https protocols
       - while A NLB routes traffic based on tcp, udp and tls ( TLS transport layer security) TLS encrypts the communication and runs over TCP
   21- Https, http,tcp (transmission control protocol),udp,ftp,


3- DOCKER SWAM
     - it is not very robust
     - It does not have capacity to scale
     - it does not allow addons while k8s you can do that via operators
   B- what are some of the best practices you have implemented when writing a docker file?
      - make of use of the .dockerignore to exclude files which are not very important for the context build.
      - use multi stage build to seperate the Build from the RUN
      - I can use multiple && to chain multiple commands  into a single RUN statement, creating fewer layers.
      - make use of a light weight image like Alpine
  c- why use && when writing a docker file? operators in RUN instructions
    - Combining commands with &&,prevents image Bloat  allows you to clean up temporary files in the same layer, reduces image size by removing unnecessary files.
    - Speeds up builds by improving caching, Docker uses layer caching to speed up builds. If you change one line, Docker invalidates the cache for that line and all subsequent layers.
    - Ensures reliable execution by chaining dependent commands.
    - The && operator ensures that the next command only runs if the previous one succeeds (returns exit code 0)

4 - WHAT HAVE YOU BEEN ABLE TO DO WITH ANSIBLE
    - It is a tool used to do automation work like 
      1- configuring of servers: install ansible , configure inventory file, test connectivity, write the playbook(), run the playbook 
          (ansible-playbook -i /etc/ansible/hosts playbook.yml
      2- deploying apps (Install ansible on the controller, define my inventory file(/etc/ansible/hosts), write the playbook(deploy.yaml), run the playbook)
      3 - provisioning of infra
   -  With the use of ansible playbook you can do a lot of work. You can use Roles to make sure that the playbook is reusable on different envs.( ansible-galaxy create "role name" )
  - Ansible works with the use of modules.  a module is a reusable, standalone script that performs a specific task or operation on a target system. 
     * You could have the 
          file module,==> is used to manage file system objects like files, directories, and symbolic links on remote hosts. It helps in tasks such as creating, deleting, modifying permissions, setting ownership, and managing symlinks.
          copy module, ==>  It is mainly used for transferring simple files, setting permissions, and ensuring file presence. Ensures idempotency (only updates the file if changes are detected).
          template module,==> Generate config files like nginx.conf, Secrets Management: Store sensitive information as templates and populate them securely at runtime.
          lineinfile module, ==> Modify a specific line in a file
          service module ==> manage service
          command module ==> run commands on remode nodes
          cron module ==> to mande cron jobs
          fetch module ==>  Fetch files from remote servers
          replace module ==> replace text in a file
          stat module ==> retriev infos about files 
          ping module ==> Check connectivity with remote hosts
          firewalld module ==>  Manage firewall rules in Firewalld
          archive module, 
          register module, 
          get url module (to be able to download file), 
          and shell module. ==> to run shell scrypt
          k8s ==> Manage Kubernetes resources
          docker_container ==> Manage Docker containers
          docker_network ==>  Manage Docker networks
          helm ==> Manage Helm charts
  5-  which module does  not respect the  principle of idempotence in ansible?
        - Command Module         user module
        - shell module
  6- How many Playbooks should run
         1 playbook can be deployed on by default 5 machines
  7- Run a Playbook in a dry run mode
          --check       will run the playbook on a dry run mode
  8- How to skip certain task in a playbook
       -- skip tags  ( to skip)
           tag   ( to include)
  9- How do you test a playbook when you write it?
       - you could use molecule or a lint to test the playbook
       - running in on dry run
  10- How do you run your playbook in production?
     - It should be running in a pipeline. You trigger a pipeline to run the playbook and execute where it is supposed to since the playbook should be kept in a source code repo
  11- How can you increase the number of playbooks to run in ANSIBLE ?
      - you can use a fork: fork=20
         ansible.cfg file
     - For servers that are not running: you use dynamic enventory which permits to add inventory in the host file
             - We have static inventory (A static inventory is a manually maintained file that lists all the managed nodes  written in YAML or JSON)  
               and Dynamic Inventory (A dynamic inventory is generated in real-time by querying external data sources, such as cloud providers, CMDBs, or other APIs .
             - an inventory defines the hosts and groups of hosts upon which Ansible commands, modules, and playbooks operate. 

  12- In which language is ansible written?
      it is written in python
  13- What is a Playbook?
       An Ansible Playbook is a YAML file that defines a series of tasks and configurations to be executed on remote or local systems.
       It is a set/list of instructions/task/ plays that have to be executed on managed nodes.
  14- By default what strategy does ansible use to excute the playbook?
      Ansible uses the linear strategy: for example if the playbook has to run on 10 servers it will execute stage1 on all servers before it can proceed.
      you can change it to parallel strategy
  15- To reduce the time of execution of ansile ?
      - you can disable to gather facts ( gather fact: false)
      - change the default execution of the playbook from linear to parallel
  16- To know the time of execution of a playbook
      You use the TIMER
  17- What is a role in ansible?
     It permits to seperate the playbook and run it in different env. You can create it using ansible_galaxy
  18- What is ansible Tower?
   - It is the enterprise version of ansible which comes with a user inteface which enable you to be able to control all the managed notes from the user inteface and you see them. 
     You  can know which  playbook failed, which playbook is still running, it gives full visibility and can permitt you to run your playbook across different teams
  19- Why Use Ansible for Provisioning?
       - Agentless (uses SSH)
       -  Supports multiple cloud providers
       - Idempotent (ensures consistent state)
       - Easily integrates with CI/CD pipelines
  20- what are some of the playbooks you have written?
     - I have writtena playbook to deploy a docker compose
     - a playbook to install nexus
     - a playbook to install jenkins
     - a playbook to add users to a system and delete users
     - 
  21- which module permits us to store variables to be able to use in another?
      - The register module:
  22- If you have a variable on the variable files how do you pass that on the playbook?
       --extra vars "name of variable"
  23- How do you solve the problem of idempotency in a playbook?
      if you want to create a file which has already been created you use ( --create ) the command will not be executed if the file exist, and  (--delete) will not delete any file if file does not exist.
  24- how do you handle secretes in ansible?
      by using ansible vault : it can be used encrypt the playbook ,it can be used to keep the keys,manage secrets , SSH keys, 
  25- when you write a playbook where do you keep it?
      Since it needs to be versioned it needs to be kept in a SRC repo and configure a pipeline to run the playbook.

5- WHY DO YOU NEED OBSERVABILITY
    Observability is very important to know the performance of our system and also before any issue occurs we should be able to have visibility of what is happening.. 
   - I have been able to monitor linux servers via node exporters
   - I have been able to monitor services/3 party app via client libraries and exporters
   - I have monitored containers via c- advisor
   - I have monitored k8s cluster via kube-state metrics 
  I was able to install prometheuse using helm charts and put in place Grafana dashboards to be able to visualize those metrics on Grafana dashboards. 
  - I have been able to send alerts to the different parties that are involved in case thresholds were exceeded.
       - firing
       - insufficient data
       - ok
  - I have sent alerts based on cpu utilization, memory usage, number of requests, and number of errors like 400 above a certain %. I've been able to set an alert for latency to know how fast my apps are responding.
   - I have been able to put in place system of logging to be able to help me know what is actually wrong with my app by consulting logs (fluend) and equally traces using open Telemetry to be able to have full visibility of the movement of the request of a client from when the request is sent and received back.

 6 - What is the git flow strategy
    - It is a strategy of maintaining 3 branches. If there's any bug, you can create the bug fix branch.
      if there's a new feature you create a feature branch. 
    - A pull is a formal approval from a superior to be able to merge a branch into the master.
    - A READme file is a file that carries information about the project.
    - To come back to a staged branch ( git starch pop)
    - git log permits to see the commit history.
    
 7- LINUX 
     - Some commands used to check network
        - nsloolup for dns resolution
        - netstat -tlup
        - ping for
        - dig for dns compliance 
    - to check users in thee system 
          /etc/passwd
    - To check resources that the system is running
           df -h, free -m
    - What is swap in linux?
       it is used to recover memory. some apps that are running but no longer active
    - What is daemon?
      It is background process running without human intervention
    - How to check if a process/ service is running?
        systemctl status "name of service"
 
 8 - AWS
      IAM is an AWS service that can be used to control who has access to the cluster and who can do what. 
    - What is an S3 service
      It's an AWS service is used to store object 
    - How do you upload objects into an S3
        1- via console ( the max size is 160 GB)
        2- or command line ( up to 50 TB)
    - how many S3 buckets can you have per AWS account?
        100 but you can apply to have up to 1000
    - What is a bucket policy? 
         An S3 bucket policy is a JSON-based access control policy that defines permissions for Amazon S3 buckets and the objects they contain.
     How do you control access into an S3 bucket?
       1-By using an Access Control List.
         This is a kind of permission given to be able to access individual objects from the bucket.
       2- A bucket policy.
          A bucket is attached on the object in the bucket.
       You can replicate items on the bucket from one AWS account to another.
    - How do you secure data on AWS?
      1- By using TLS, and SSL that permit you to encrypt data on transit
      2- You can encrypt AT REST using server-side encryption or a client side encryption.
    - AWS serverless services
        - lamda 
        - API gate ways
        - fargates
        - s3
    - What is AWS LAMBDA?
       This a compute service that you can use to run code without provisioning or creating any server. 
       It is one of the fast way of running apps now because individuals don't want to carry the heavy lifting of managing infra. 

    - What is A LAMDA FUNCTION? It the app code that can be written in python, nodejs, java, C-sharp, go
      It is a code that have been written to permit the job to be done. A dev can write a code but they don't want to provision any server/resources to deploy the app 
       so they will upload the code to a lambda function.
      Ones the lambda functions takes the code, you need to define the memory that your app will use to run on and AWS will create an env taking into consideration
        the memory needed by the app, the CPU, the dependencies which it needs to download. 
      This env is also known as an execution env. Ones the env is ready that code will only run base on a trigger, then the a destination.
       An S3 bucket can trigger a lambda function. e.g pictures are dropped in an S3 bucket. Based on a PUT EVENT on an S3 bucket, 
      it's going to trigger a lambda function but for it to happen, the resource that needs to trigger the lambda functions needs a resource base policy. 
         There must be a policy that gives the S3 bucket permission to act on the lambda function. 
      The function will run based on trigger, convert the picture and drop it in its destination, which could be another S3 bucket. For it to drop it in the S3 bucket,
         the lambda function should have an execution role ( the action which the lambda function has on the resource). 
     - It could also be triggered by a synchro nose evocation is where, when the lambda function is evoked it will need to wait to get a response and send 
         it back but an asynchrony nose evocation is when the lambda function is evoked it goes and deliver without waiting for a response. 

    1- IaS: Virtualized computing resources (servers, storage, networking) on demand. This is where a commpany wants to run their infrastructure but they don't want to 
           create their own servers, they don't want to build their own data centers etc they want to use the cloud. 
           Aws is an example of IaS because you can create virtual machines and use them to run your app or someone who wants to buld a house but doeesn't want to 
           look for land and utility providers.
         User Responsibility: Full control over OS, storage, and apps. You manage OS, applications, and data, Highly customizable. e.g AWS EC2, Azure VM, Google Compute Engine.
         Used Cases: Hosting websites, running VMs, data storage. 
    2- PaaS: Platform to build and deploy applications without managing the underlying infrastructure e.g that same person wo wants to build a house, he decides tat he will look 
             for a contractor or a company thhat can do everythng and they build the hhouse and and him the keys.
         User Responsibilty: Limited control over OS, focuses on app development. You manage applications and data. Limited customization. e.g AWS Elastic Beanstalk, Azure App Service, Google App Engine,EKS
         used Cases: Developing and testing apps, middleware.
    3- SaaS: Ready-to-use software available over the internet. No control over infrastructure or platform. Provider manages everything.
          User Responsibility: No customization of infrastructure.
          Used Cases: Collaboration tools, CRM, email services. e.g Google Workspace, Dropbox, Salesforce, Slack. (Here you buy already customized app and run it on the platform)
    4- IaC: writing a code to provision infrastructure as a service.

   5- Assume you have an app and that app is running and a company wants to deploy that app and is asking you what kind of cloud services  are you going to use for you to deploy your app.
      How are you going to deploy the app? The code have been written.
    - I will first fine out what the company actually wants to deploy. What kind of app do they want to deploy. 
      Is it a kind of app tat will be running continously or it's an app that will be executed it stops and can be executed again. In these stuations AWS as serverless services and services that are not serverless.
      It's going to depend on wether the company wants to have control of their infrastructure or they want to give their infrastucture to AWS to handle so that they focus on their business logic
      (making sure that the app is good rather than managing the infrstructure). If the app will need a serverless service which will not run for more than 15m then we can use Lambda function. But if it will run for long 
      we can use EC2 compute services. The EC2 compute services can run the app on the virtual machines.Those app are they containerized app? If they are containerized app which means they will be converted to image, 
      AWS has container services that are both serverless and not serverless. AWS has ECS which is vendor login (no external addon) and EKS which is not. The EKS has different options. The question now will be does your
     Dev team have expertise to be able to manage thee cluster in terms of upgrade, patching (managed EKS, semi Managed EKS, fully managed where the worker nodes will be in the form of fargates which are serverless services)
      The next thing to consider is how will the app be made highly available. If I am using virtual machines I need to make sure that my app is deployed in a region with more than 1 availability Zone because in case one has
      a problem the other will still keep serving my traffic. I need to see how I can secure my cluster. Securing from the cloud pespective is making sure that I have curved out a virtual private cloud where my resources will be 
      running separate from other companies. I'm going to curve out the VPC, decide on the cidr block that my app is going to use in terms of provisioning the IP address that the servers are going to be using. I'm going 
      to have IGW that acts as entery point of internet into my cluster. I will make sure I put  in place a firewall around my subnets (NACL which are stateless) hence I will have to configure inbound role and ourbound role,
      SG aroud my EC2 instance which are statefull (configure inbound role and no outbound role). Ones this is done I will put in place a system of one way flow into my cluster which is a NAT gateway becaue from time to time
      I need to update my k8s cluster with the stable version to fetch the package from the internet for update of my cluster. I need to think if my app needs a database i need to ask myself if its  a Relational database or non-relational
      database. The issue here how to I make users to be able to access my app running inside the cluster in AWS. I need to be able to use a loadbalancer because I am having 2 clusters running in 2 availability zones so users
      cannot access the clusters directly. The cluster needs to have 1 entry point which is a loadbalancer. A loadbalancer will sit ontop of the cluster to be able to loadbalance request on the different clusters. I'm
      going to configure a loadbalancer to be able to loadbalnce to the backend or IP addresses and that will be an ALB because its a kind of layer 7 support that does host based routing. Apart from that my app should have a domain name
     I should have a DNS which is route 53. The route 53 is going to be resolving the domain name into IP addresses. Once that is provisioned I'm going to create A records into my route 53 that is going to be resolving domaiin name into IP
      addresses. In the route 53 i need to configure the TTL making sure that for high availability, request made from one client should be routed to the same server over a period of time to increase the spped and response 
     time of our app. To equally increase the response time I can put in place a CDN ie cloudfront which is going to be catching our request based on our users location e.g request will be catched at edge location to reduce 
     the time it takes to make a request and get a response. If my app will have to store some data or objects like photos, files, logs i could be able to usse S3 bucket which my app should be able to connect to the S3 bucket.
     I need to be able to put in place IAM role to permit the EC2 instances where my apps are running to be able to talk to the S3 bucket, I need to configure bucket policy and access roles on the bucket to permit tha app to
    be able to talk to another app.For those who have access to my AWS account I need to put in place IAM to be able to control who have access  to each resouurce by attaching roles to users and giving them permission. I would
    equally put in place an ASG to be able to scale my resources when workloads increase (set threshold in confirmaty with cost reduction). if my applications are running in the cloud and want to talk to an unpremise application
    i will  ensure  connectivity viia a VPGW (virtual Prvate Gateway) whiich links our unpremise database on cloud application by creatiing a secure VPN tunnel, IPsec tunnel that encrypt the network. I need to also secure the
     data that are running wiith my applications by putting in place SSL cert for encryption AT REST, In TRANSIT ... by using key management services.  Taking into consideration cost of our infratructure, nowadayss companiess are
    work with budget and you need to make sure that you are working within the budget.( if the objects they are accessing are logs then i will make use of a lifecycle policy sso that after 30 days the objects will move to another
    part of our bucket to prevent cost. I can equally set alarm with the AS if the have to watch our resources). We could equally store app files in EBS volumes( if  the app has to persist data) rather than using instance store.
    because thhe processing will  be fast since its not persisting data.
 
   6- How  do you ensure security in AWS?
    - security of my AWS account :This is done using Multi-factor authentication (email and password,  a device own my the owner of the accoumt)
                            IAM : HHelps to giive users least privilege meaning thhey should hhave access to only resorces are they working on. You create users, you create policies and attach to them to users to apply on resources.
    - security of our data: using key managment services to be able to secure our data ( AT app level, In Transit and At Rest)
                           We can also secure attacks where if a user is sending heavy load that our app cannot handle e.g DDoS, we could use AWS AdvanceSheild which prevent/sheild against Cross site scipting, SQL injection.
    - At the level of VPC: you can put in place WAF 
    - At the level of Subnets: NacL
    - At the level of EC2 instances: SG (controlling which IP address can access which port)
    - To be able to store secrets in AWS: we can use AWS parameter store (just like ansible vault)  which provides a rotation of the secrets. It is different from KMS( KMS you can configure your rotation) 
        because it provides automatic rotation of keys

  7- when we talk of relational database 
      - you need to look at the engine
 8- IP address types
     - multicast: One-to-many communication (efficient) e.g Video streaming, IPTV
     - Broadcast: One-to-all communication e.g ARP requests, DHCP
     - unicast : A single IP address is advertised to one communication, and the closest server responds.e.g Web browsing, emails
     - anycast: A single IP address is advertised from multiple locations, and the closest server responds. e.g CDN networks (Cloudflare, Akamai), DNS services.
  9- A reversed proxy: is a server that is sitting on another server on premise (e.g HAProxy or NGINX) 
  10- What are the different types of vvirtulization in AWS?
       - virtual machines: They permit you to be able to run your applications on bare meters. They have their own operating systems that does not depend on the hosst operating system and therefore it becomes heavy
       - Containers: They depend on the operating system of the host and since the contaiiner must be isolated from the host so that the file system of  thee  host doesnot have access to the filesystem of the container
              because they are sharing the same kernel there could be a security concern
  11- Types of migration> you need a service which is similar  to the service you are currently using. You also need a shema conversion tool which will convert the schema(kind of engine) of your tool to the schema of what you will be using) 
       - migrating app from on premise(openshift own by RedHeart) to cloud ( AWS direct connect which is very fast, snowball which you order from AWS which will be available in 2 weeks) because the company wants to go 
          global, reduce cost, and scale repidly 
       - migrating and app from one platform to another platform e.g from monolilth to micro services. from an old unix platform to a linux platform
       - move to opn migration:

9- Terraform
    - How do you create infrastructure?
        I use IaC to be  able to create infrastructure.
    - What is terraform show: 
         it shows all the  information in the state file
    - What are the elements of the hashicorp  configuration in the terraform  block?
        we use the terraform block to be able to configure our project.
    -  What are the differrent blocks used to configure resorces
        1- resource block
            - the type of resource
            - the label : used  to uniquely identify a resource
        2- Data Source Block
        3- Varables block : we can customize our variables which we want to use in our CF
           - varable name
           - type
           - description
        4- output Block : it's used to exposed information to the outside world
        5- The Local block : it permits to create a temporal variable inside a function.
        6- Module Blocks: It's a piece of a reusable code that can be imported in our current project 
            - they have labels
  10- In k8s what were you scaling?
      - you can scale the pods
      - you can scale the cluster
  11- What is vertical auto-scaling ? scaling up or down
       Involves increasing or decreasing the power of a single server or instance. Adds more CPU, RAM, or storage to the existing machine when demand increases.
       Examples: Database servers, monolithic applications. disadvantage is has  a limit to how much you can scale and may lead to down time when scaling up and down.
  12- What is horizontal auto-scaling ? scaling in or out
       Involves adding or removing instances (servers) to distribute the load.
       Examples: Web applications, APIs, and stateless services

    Vertical scaling = Hiring a super chef to cook faster.
    Horizontal scaling = Hiring multiple chefs to handle more orders simultaneously.
13- what was giving you the metric server?
     cloudwatch was watching the servers and the metric server which was installed in the cluster was collecting the metrics about the CPU usage (kubectl top), alert manager will send the alert of the cpu utilization.
14- In which situation are you going to do a horizontal pod auto-scaling and in which situation are you going to do a horizontal cluster auto-scaling?
     - For horizontal pods auto-scaling you are scaling pods when there's increase in workloads and the pods are consuming resources ( we have min, max and desired) if the pods have used all the resources in 
       the cluster and cluster can no longer have resources because there's way they will consume resources then you scale the  cluster to create more worker nodes so that more pods  can be scheduled on the worker nodes
       until the horizontal auto-scaling has reached its max and there's still increase in workloads then you can proceed to scale the cluster so that you can have more worker nodes to schedule pods to handle incarese in traffic
       you start by scaling pods based on the replicas that have been defined. you have to scaled the pods first until the avearge resources defined in the  cluster have been exceeded then you scale the cluster.
15- what is white box monitoring?
    is having complete visibilty of the system internally and having and output of what is happening. you can use instrumentation to do a white box monitoring by including a prometheus code in the app 
    code base so that promethuesss can collect details of what is happening inside the app and you are able to have it externally
    while blank box montitoring, monitors the system superficially using tools like nagios
16- what  do pod use to communicate which each other?
    they communicate which each other using cluster IP.
17- How do 2 containers inside a pod communicate with each other?
    using a shared namespace to communicate with each other produces by a pause container?
18- Have youu had a situation where you had 2 containers running inside a pod?
    - init container
    - sidecar container
    - Ambassador container: it is a proxy container used to enhance communication wheree the maiin container does not have means to communiicate
    - Adapter contaiiner : used to formate metrics
19- How to you make sure that teams don't exceed their resources defined in each namespace?
    use resources request and resouces quotas
20- 
===================================================================================================================================================================================================================


